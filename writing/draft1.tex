% document layout
\documentclass{article}
\usepackage[margin=1.3in]{geometry}
\usepackage[utf8]{inputenc}

% packages
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mdwtab}
\usepackage{syntax}
\usepackage{stmaryrd}
\usepackage{mathtools}
\usepackage{mathpartir}
\usepackage{listings}
\usepackage{float}

\floatstyle{boxed} 
\restylefloat{figure}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\renewcommand{\syntleft}{}
\renewcommand{\syntright}{}
\setlength{\grammarparsep}{10pt}
\setlength{\grammarindent}{10em}
\newcommand{\indalt}[1][2]{\\\hspace*{-1.2em}\textbar\quad}
\newcommand{\rname}[1]{\textsc{\footnotesize #1}}
\newcommand{\LNLD}{\text{LNL}_\text{D}}
\newcommand{\ok}{\textbf{ok}}
\newcommand{\pure}{\textbf{pure}}
\newcommand{\sort}{\textbf{sort}}
\newcommand{\type}{\textbf{type}}
\newcommand{\refl}{\text{refl}}
\newcommand{\letin}[3]{$\text{let }#1\text{ := }#2\text{ in }#3$}
\newcommand{\new}{\text{new}}
\newcommand{\free}{\text{free}}
\newcommand{\get}{\text{get}}
\newcommand{\set}{\text{set}}
\newcommand{\subst}{\text{subst}}
\newcommand{\utype}{\overset{U}{:}}
\newcommand{\ltype}{\overset{L}{:}}
\newcommand{\stype}[1]{\overset{#1}{:}}
\newcommand{\step}{\leadsto}
\newcommand{\pstep}{\leadsto_p}

% title and author
\title{The Calculus of Linear Constructions}
\author{Qiancheng Fu}

% document body
\begin{document}
  \maketitle 
  \begin{abstract}
    The Calculus of Linear Constructions (CLC) is an extension of the Calculus of Constructions (CC) with linear types. Specifically, CLC extends CC with a hierarchy of linear universes that precisely controls the introduction and elimination of its term level inhabitants. We study the meta-theory of CLC, proving that it is a sound logical framework for reasoning about resource. CLC is highly backwards compatible with CC, allowing CLC to enjoy the decades of CC research. We have formalized all major results in the Coq Proof Assistant. We extend CLC with linear inductive types and show that CLC as a programming language enables the manipulation of mutable data structures in a principled way.
  \end{abstract}

  \section{Introduction}
  The Calculus of Constructions (CC) is a dependent type theory introduced by Coquand and Huet in their landmark work \cite{cc}. In CC types can depend on terms, allowing one to write fine-grain propositions as types. Today, CC and its variations CIC \cite{cic} and ECC \cite{ecc} lie at the core of popular proof assistants such as Coq \cite{coq}, Agda \cite{agda}, Lean \cite{lean}, and others. These theorem provers have found great success in the fields of software verification \cite{compcert,deepspec}, and constructive mathematics \cite{four-color,schemes}. 
  
  However, due to its origins as a logical framework for constructive mathematics, it is quite difficult for CC to encode and reason about resources. Intuitively, a mathematical theorem can be applied an unrestricted number of times. Comparatively, the usage of resources is more limited. For example, if we encode Girard's classical example \cite{girard95} of purchasing cigarettes literally into CC as a function of type:

  \begin{equation*}
    Money \rightarrow Camels \times Marlboro
  \end{equation*}

  \noindent The customer will still maintain full ownership of their money after paying the vendor. Unless the vendor is exceedingly generous, we are faced with the crime of counterfeiting. Users of proof assistants based on CC often need to embed external logics into CC \cite{bedrock} to provide additional reasoning principles for dealing with resource. The design and embedding of these logics is a difficult problem in its own right, requiring additional proofs to justify its soundness. We propose an alternative solution: extend CC with linear types.

  Linear Logic is a substructural logic introduced by Girard in his seminal work \cite{girard}. Girard notice that the Weakening and Contraction rules of Classical Logic when restricted carefully, gives rise to a new logical foundation for reasoning about resource. Wadler \cite{wadler1990,wadler1991} first notice that an analogous restriction to variable usage in simple type theory leads to a linear type theory, where terms respect resources. A term calculus for linear type theory was later realized by Abramsky \cite{abramsky1993}. Benton \cite{benton1994} investigates the ramifications of the ! exponential in linear term calculi, decomposing it to adjoint connectives $F$ and $G$ that map between linear and non-linear judgments. Programming languages \cite{l3,ats,linear-haskell} featuring linear types have also been implemented, allowing programmers to write resource safe software in practical applications. The success of integrating Linear Logic with simple type theory exposes a tantalizing new frontier of integrating linearity with richer type theories.

  Work have been done to extend dependent type theories with linear types. Cervesato and Pfenning extends the Edinburgh Logical Framework with linear types \cite{lf,llf}, being the first to demonstrate that dependent types and linear types can coexist within a type theory. V\'{a}k\'{a}r \cite{vakar14} gives a categorical semantics for linear dependent types.  Krishnaswami et al. present a dependent linear type theory \cite{neel15} based on Benton's early work of mixed linear and non-linear calculus, demonstrating the ability to internalize imperative programming the style of Hoare Type Theory \cite{htt}. Luo et al. introduce the property of essential linearity, and a mixed linear/non-linear context, describing the first type theory that allows types to depend on linear terms. Based on initial ideas of McBride \cite{nothing}, Atkey's Quantitative Type Theory (QTT) \cite{qtt} uses semi-ring annotations to track variable occurrence, simulating irrelevance, linear, and affine types within a unified framework. The Idris 2 programming language \cite{idris2} implements QTT as its core type system.

  We propose a new linear dependent type system - The Calculus of Linear Constructions (CLC). Compared to preexisting approaches for integrating linear types and dependent types, CLC offers a ``minimally invasive'' approach to extending CC with linear types. This allows for a straightforward embedding of CLC into CC, and lifting of CC into CLC, endowing CLC with the fruits of decades of CC research. We accomplish this by adding extra universes $L$ of linear types with cumulativity parallel to the universes $U$ of non-linear types. Universe information is propagated by an indexed typing judgment down to the term level, controlling the usage of Weakening and Contraction rules. We further extend CLC with inductive types, showing that as a programming language it can manipulate mutable data structures in a principled way. We have formalized all major results in Coq, and implemented a prototype in OCaml. 

  \medskip

  \noindent \textbf{\textit{Contributions}}: 
  Our contributions can be summarized as follows.
  \begin{itemize}
    \item Fist, we describe the Calculus of Linear Constructions, an extension to the Calculus of Constructions with linear types. The integration of linear types and dependent types allows CLC to directly and precisely reason about resource.
    \item Next, we study the meta-theory of CLC directly, showing that it satisfies the standard properties of confluence, progress, regularity, and preservation. 
    \item We observe that CLC is highly backwards compatible with CC. We construct a reduction preserving embedding of CLC into CC, showing that CLC is consistent. 
    \item All major results have been formalized and proven correct in the Coq Proof Assistant with help from the Autosubst \cite{autosubst} library. To the best of our knowledge, our development is the first machine checked formalization of a linear dependent type theory.
    \item Furthermore, we extend CLC with linear inductive data, demonstrating that as a programming language, CLC can safely manipulate mutable data structures.
    \item Finally, we give a implementation extended with user definable linear and non-linear data types. Algorithmic type checking employed by the implementation streamlines the process of writing CLC.
  \end{itemize}

  \section{Core Type Theory}
  In the efforts of integrating dependency and linearity, there are two predominant schools of thought: \textit{Types can only depended on non-linear terms} and \textit{Types can depend on all terms}. 
  
  Advocates of restricted dependency argue that the restriction is both intuitive and a necessity. A natural example of linear types depending on a non-linear terms is that of the length indexed random access array. Term dependency on array length statically prevents out-of-bounds array access whilst linearity ensures proper memory management. In the converse situation of types depending on linear terms, one immediately faces a dilemma: \textit{Do linear terms at the type level possess weakening and contraction?}.
  
  If one answers yes, then the linear terms at the type level are defacto non-linear. In this case, a language with restricted dependency can simulate linear dependency by depending on non-linear terms that reflect the shape of linear terms\cite{ldqt}, erasing the advantage of increased type level expressivity. All prior works featuring linear dependency fall into this category, for good reason: \textit{the alternative is worse}.
  
  If type level linear terms do not possess weakening and contraction, types with linear dependencies themselves are linear. This breaks the very notion of the typing judgment as even $\alpha$-equivalent terms are not allowed to possess the same type. Though the idea of such a type system is deeply interesting, the philosophical burden is prohibitive.
  
  Our language falls firmly in the former category. Besides the reasons listed above, restricted dependency gives a clear distinction between linear and non-linear terms, a property we believe will aid in the compilation to high performance code.
  
  \subsection{Syntax}
  The syntax of the core type theory is presented in Figure \ref{syntax}. Our type theory contains two sorts, $U$ and $L$ for denoting the sort of non-linear types and linear types respectively. An important fact to note is that our language possess the ``Type-in-Type" axiom in the form of $\Gamma \vdash U \utype U$ which is well known to be logically unsound. We chose this design deliberately as we are willing to sacrifice logical soundness for increased expressivity and convenience. We forsee no issue in extending the core language with a hierarchy of sorts to enforce logical soundness.

  \begin{figure}[H]
    \caption{Syntax}
    \centering
    \begin{minipage}{0.8\linewidth}
    \begin{grammar}
      <$s, t$> ::= $U$ | $L$ \phantom{| $x$} \hspace*{8em} sorts

      <$m, n, A, B, C$> ::= $U$ | $L$ | $x$  \hspace*{8em} expressions
      \indalt $(x : A) \rightarrow_s B$ | $A \rightarrow_s B$
      \indalt $(x : A) \multimap_s B$ | $A \multimap_s B$
      \indalt $\lambda x. n$ | $m\ n$

      <$v$> ::= $U$ | $L$ | $x$ \hspace*{8em} values
      \indalt $(x : A) \rightarrow_s B$ | $A \rightarrow_s B$
      \indalt $(x : A) \multimap_s B$ | $A \multimap_s B$
      \indalt $\lambda x. n$
    \end{grammar}
    \end{minipage}
    \label{syntax}
  \end{figure}

  A clear departure of our language from standard presentations of type theory is the presence of four function types: $(x : A) \rightarrow_s B$, $A \rightarrow_s B$, $(x : A) \multimap_s B$, $A \multimap_s B$. The reason for these variants is the decoupling of function linearity from the linearity of function domain and co-domain. Instead, the linearity of functions is determined by the linearity of the closure it forms. For example, a function with linear input and output type can itself be non-linear if its free variables have non-linear type, indicating this function can be applied repeatedly without duplication of linear variables. In practice, these function types can be consolidated to just $(x : A) \rightarrow_s B$ and $(x : A) \multimap_s B$ by treating $A \rightarrow_s B$ and $A \multimap_s B$ as non-binding special cases. For the sake of clarity, we leave them as four.

  \subsection{Context and Structural Judgments}
  The context of our language employs a mixed linear/non-linear representation in the style of Luo\cite{luo}. Variables in the context are annotated to indicate whether they are linear or non-linear. A non-linear variable is annotated as $\Gamma, x \utype A$, whereas a linear variable is annotated as $\Gamma, x \ltype A$. 
  
  We define a $merge$ relation to combine two mixed contexts $\Gamma_1$ and $\Gamma_2$ by performing contraction on shared non-linear variables. For linear variables, the $merge$ relation is defined if and only if each variable occurs uniquely in one context and not the other. The definition of $merge$ allows contraction for non-linear variables whilst forbidding it for linear ones.

  An auxiliary judgment $\pure$ is defined to assert that a context $\Gamma$ does not contain linear variables. In other words, all variables found in a pure context are of the form $\Gamma, x \utype A$. A restriction function $\overline{\Gamma}$ is defined that removes all linear variables from context $\Gamma$. The result of restriction is a pure subset of the original context.
  
  \begin{figure}[H]
    \caption{Structural Judgments}
    \begin{mathpar}
      \inferrule
      { }
      { \epsilon \ \ok }
      \rname{Ok-$\epsilon$}

      \inferrule
      { \Gamma\ \ok \\ 
        \overline{\Gamma} \vdash A \utype U }
      { \Gamma, x \utype A\ \ok }
      \rname{Ok-U}

      \inferrule
      { \Gamma\ \ok \\ 
        \overline{\Gamma} \vdash A \utype L }
      { \Gamma, x \ltype A\ \ok } 
      \rname{Ok-L}
      \\

      \inferrule
      { }
      { \epsilon\ \pure }
      \rname{Pure-$\epsilon$}

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash A \utype U }
      { \Gamma, x \utype A\ \pure }
      \rname{Pure-U}
      \\

      \inferrule
      { }
      { merge\ \epsilon\ \epsilon\ \epsilon }
      \rname{Merge-$\epsilon$}

      \inferrule
      { merge\ \Gamma_1\ \Gamma_2\ \Gamma }
      { merge\ (\Gamma_1, x \utype A)
             \ (\Gamma_2, x \utype A)
             \ (\Gamma, x \utype A) }
      \rname{Merge-U}

      \inferrule
      { merge\ \Gamma_1\ \Gamma_2\ \Gamma \\
        x \notin \Gamma_2 }
      { merge\ (\Gamma_1, x \ltype A)
             \ \Gamma_2
             \ (\Gamma, x \ltype A) }
      \rname{Merge-L1}

      \inferrule
      { merge\ \Gamma_1\ \Gamma_2\ \Gamma \\
        x \notin \Gamma_1 }
      { merge\ \Gamma_1
             \ (\Gamma_2, x \ltype A)
             \ (\Gamma, x \ltype A) }
      \rname{Merge-L2} 
    \end{mathpar}
    \begin{align*}
      \overline{\epsilon} = \epsilon
      \hspace*{4em}
      \overline{\Gamma, x \utype A} = \overline{\Gamma}, x \utype A 
      \hspace*{4em}
      \overline{\Gamma, x \ltype A} = \overline{\Gamma}
    \end{align*}
    \label{structural}
  \end{figure}

  \subsection{Type Formation} \label{tyformation}
  Typing judgments in our language take on the form $\Gamma \vdash m \stype{s} A$ where $s$ is an indexing sort that is either $U$ or $L$. Intuitively, this judgment states that expression $m$ has type $A$ and sort $s$ under context $\Gamma$. An expression of sort $L$ is linear, and sort $U$ is non-linear. Surprisingly, duplication of non-linear expression is not immediately allowed, only non-linear \textit{values} can be safely duplicated. We will discuss this subtlety later in Section \ref{subst}.
  
  Type formation rules are presented in Figure \ref{type}. Rules worth discussing in detail are the \rname{L-Axiom} and the rules for constructing various function types. For \rname{L-Axiom}, the sort of all linear types $L$ itself is of non-linear sort $U$. This means that a linear type can be freely used, avoiding the philosophical trappings discussed earlier.

  The function types $(x : A) \rightarrow_s B$ and $A \rightarrow_s B$ represent non-linear functions, where the subscript $s$ is the sort of co-domain $B$. Functions of these types contain no linear free variables, thus can be applied repeatedly without duplication of linear resources. The difference between the two is that $(x : A) \rightarrow_s B$ allows co-domain $B$ to depended on input $x$ of non-linear domain $A$. For $A \rightarrow_s B$, the domain $A$ is linear so $B$ is not allowed to depend on the function input.

  The variants $(x : A) \multimap_s B$ and $A \multimap_s B$ represent linear functions. Functions of these types may contain linear free variables, thus cannot be applied repeated without duplication of linear variables. Similar to the non-linear versions, the difference between $(x : A) \multimap_s B$ and $A \multimap_s B$ lies in the allowance of dependency on function input for non-linear domain types.

  \begin{figure}[H]
    \caption{Type Formation} 
    \begin{mathpar}
      \inferrule
      { \Gamma\ \pure }
      { \Gamma \vdash U \utype U } 
      \rname{U-Axiom}

      \inferrule
      { \Gamma\ \pure }
      { \Gamma \vdash L \utype U } 
      \rname{L-Axiom}
      \\

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash A \utype U \\ 
        \Gamma, x \utype A \vdash B \utype s }
      { \Gamma \vdash (x : A) \rightarrow_s B \utype U } 
      \rname{U-Prod}

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash A \utype L \\ 
        \Gamma \vdash B \utype s }
      { \Gamma \vdash A \rightarrow_s B \utype U } 
      \rname{Arrow}
      \\

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash A \utype U \\ 
        \Gamma, x \utype A \vdash B\ \utype s }
      { \Gamma \vdash (x : A) \multimap_s B \utype L } 
      \rname{L-Prod}

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash A \utype L \\ 
        \Gamma \vdash B \utype s }
      { \Gamma \vdash A \multimap_s B \utype L } 
      \rname{Lolli}
    \end{mathpar}
    \label{type}
  \end{figure}

  \subsection{Term Formation} \label{teformation}
  The term formations rules are presented in Figure \ref{term}. 

  The rules \rname{U-Var} and \rname{L-Var} state that a variable's type is determined by its context. In the case of \rname{L-Var}, the linear variable $x$ must be the only linear variable within its context. This enforcement of uniqueness eliminates the weakening rule for linear variables as redundant linear variables in the context will prevent the usage of \rname{L-Var}.

  The \rname{Conv} rule is standard with regards to prior dependent type theory literature. Two types $A$ and $B$ are treated equivalently if they are $\beta\eta$-convertible. Our treatment of the convertibility relation $A \equiv B$ differs from standard due to our call-by-value semantics. We give a detailed account of this in the next section.

  For the non-linear function formation rules \rname{U-$\lambda_1$} and \rname{U-$\lambda_2$}, the context $\Gamma$ is asserted to be pure. The purity of $\Gamma$ ensures that no linear variables occur within function body $n$, allowing the function to be freely used without duplication of linear variables. For the linear function formation rules \rname{L-$\lambda_1$} and \rname{L-$\lambda_2$}, the restriction on $\Gamma$'s purity is lifted. Linear variables within $\Gamma$ are allowed to occur freely within function body $n$. However, the tradeoff is that these linear functions may only be applied once, as multiple uses may result in duplication of its linear free variables.

  In the application rules $\rname{U-App-1}$ and $\rname{L-App-1}$, the argument $n$ is a non-linear expression that may contain linear free variables. If reduction is performed naively, substitution of $n$ into the function body may cause duplication of these variables. Our operational semantics and value soundness lemma guarantee that substitution will not duplicate linear variables.

  Furthermore, in the \rname{U-App-1} and \rname{L-App-1} rules, the input expression $n$ is not directly substituted into the dependent co-domain $B$ as this may introduce linear variables into types. Instead, a $\lambda$-abstraction is formed around $B$ and applied to $n$. This delays $\beta$-reduction until $n$ can be evaluated into a value.

  \begin{figure}[H]
    \caption{Term Formation}
    \begin{mathpar}
      \inferrule
      { \Gamma\ \pure}
      { \Gamma, x \utype A \vdash x \utype A } 
      \rname{U-Var}

      \inferrule
      { \Gamma\ \pure }
      { \Gamma, x \ltype A \vdash x \ltype A } 
      \rname{L-Var}

      \inferrule
      { \Gamma \vdash m \stype{s} A \\ A \equiv B }
      { \Gamma \vdash m \stype{s} B } 
      \rname{Conv}
      \\

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash (x : A) \rightarrow_s B \utype U \\ 
        \Gamma, x \utype A \vdash n \stype{s} B }
      { \Gamma \vdash \lambda x . n \utype (x : A) \rightarrow_s B }
      \rname{U-$\lambda_1$}
      \\

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash A \rightarrow_s B \utype U \\
        \Gamma, x \ltype A \vdash n \stype{s} B }
      { \Gamma \vdash \lambda x . n \utype A \rightarrow_s B }
      \rname{U-$\lambda_2$}
      \\

      \inferrule
      { \overline{\Gamma} \vdash (x : A) \multimap_s B \utype L \\ 
        \Gamma, x \utype A \vdash n \stype{s} B }
      { \Gamma \vdash \lambda x . n \ltype (x : A) \multimap_s B }
      \rname{L-$\lambda_1$}
      \\

      \inferrule
      { \overline{\Gamma} \vdash A \multimap_s B \utype L \\ 
        \Gamma, x \ltype A \vdash n \stype{s} B }
      { \Gamma \vdash \lambda x . n \ltype A \multimap_s B }
      \rname{L-$\lambda_2$}
      \\

      \inferrule
      { \Gamma_1 \vdash m \utype (x : A) \rightarrow_s B \\
        \Gamma_2 \vdash n \utype A \\
        merge\ \Gamma_1\ \Gamma_2\ \Gamma }
      { \Gamma \vdash m\ n \stype{s} (\lambda x.B)\ n }
      \rname{U-App-1}
      \\

      \inferrule
      { \Gamma_1 \vdash m \utype A \rightarrow_s B \\
        \Gamma_2 \vdash n \ltype A \\
        merge\ \Gamma_1\ \Gamma_2\ \Gamma }
      { \Gamma \vdash m\ n \stype{s} B }
      \rname{U-App-2}
      \\

      \inferrule
      { \Gamma_1 \vdash m \ltype (x : A) \multimap_s B \\
        \Gamma_2 \vdash n \utype A \\
        merge\ \Gamma_1\ \Gamma_2\ \Gamma }
      { \Gamma \vdash m\ n \stype{s} (\lambda x.B)\ n }
      \rname{L-App-1}

      \inferrule
      { \Gamma_1 \vdash m \ltype A \multimap_s B \\
        \Gamma_2 \vdash n \ltype A \\
        merge\ \Gamma_1\ \Gamma_2\ \Gamma }
      { \Gamma \vdash m\ n \stype{s} B }
      \rname{L-App-2}
    \end{mathpar}
    \label{term}
  \end{figure}

  \subsection{Reduction and Equality} \label{reduction}
  Figure \ref{single} presents the call-by-value operational semantics that we have eluded to. The rules defining the single step relation $\step$ are completely standard.

  For dependently typed languages, terms can appear at the type level. A definitional equality judgment is required to identify types beyond simple $\alpha$-equivalence. This is usually accomplished by normalization and comparing normal forms. Due to the complications brought on by linearity and substitution hinted at in \ref{teformation}, standard normalization techniques cannot be directly applied. Unfortunately, the single step relation $\step$ defined previously is not sufficient either as binders block evaluation. Following the footsteps of the Trellys project\cite{trellys}, we define a call-by-value parallel step relation $\pstep$ that evaluates under binders. We use the transitive reflexive closure of $\pstep$ to define definitional equality.

  \begin{figure}[H]
    \caption{Single Step Reduction}
    \begin{mathpar}
      \inferrule
      { }
      { (\lambda x.n)\ v \step [v/x]n }
      \rname{S-U-$\beta$}

      \inferrule
      { }
      { (\lambda x.n)\ v \step [v/x]n }
      \rname{S-L-$\beta$}

      \inferrule
      { m \step m' }
      { m\ n \step m'\ n }
      \rname{S-App-L}

      \inferrule
      { n \step n' }
      { v\ n \step v\ n' }
      \rname{S-App-R}
    \end{mathpar}
    \label{single}
  \end{figure}

  \begin{figure}[H]
    \caption{Equality and Parallel Reduction}
    \begin{mathpar}
      \inferrule
      { m_1 \pstep^* n \\ m_2 \pstep^* n }
      { m_1 \equiv m_2 : A }
      \rname{Join}

      \inferrule
      { }
      { x \pstep x }
      \rname{P-Var}

      \inferrule
      { }
      { U \pstep U }
      \rname{P-U}

      \inferrule
      { }
      { L \pstep L }
      \rname{P-L}

      \inferrule
      { n \pstep n' }
      { \lambda x.n \pstep \lambda x.n' }
      \rname{P-$\lambda$}

      \inferrule
      { n \pstep n' }
      { \lambda x.n \pstep \lambda x.n' }
      \rname{P-$\lambda$}

      \inferrule
      { m \pstep m' \\ n \pstep n' }
      { m\ n \pstep m'\ n' }
      \rname{P-App}

      \inferrule
      { n \pstep n' \\ v \pstep v' }
      { (\lambda x.n)\ v \pstep [v'/x]n' }
      \rname{P-U-$\beta$}

      \inferrule
      { n \pstep n' \\ v \pstep v' }
      { (\lambda x.n)\ v \pstep [v'/x]n' }
      \rname{P-L-$\beta$}

      \inferrule
      { A \pstep A' \\ B \pstep B' }
      { (x : A) \rightarrow_s B \pstep (x : A') \rightarrow_s B' }
      \rname{P-U-Prod}

      \inferrule
      { A \pstep A' \\ B \pstep B' }
      { A \rightarrow_s B \pstep A' \rightarrow_s B' }
      \rname{P-Arrow}

      \inferrule
      { A \pstep A' \\ B \pstep B' }
      { (x : A) \multimap_s B \pstep (x : A') \multimap_s B' }
      \rname{P-L-Prod}

      \inferrule
      { A \pstep A' \\ B \pstep B' }
      { A \multimap_s B \pstep A' \multimap_s B' }
      \rname{P-Lolli}
    \end{mathpar}
    \label{parallel}
  \end{figure}

  \subsection{Meta Theory}
  We have proven the type soundness of our language in the form of \textit{progress} and \textit{preservation} theorems. The proofs have been formalized in Coq with help from the Autosubst\cite{autosubst} library.

  \subsubsection{Step and Parallel Step}

  The following lemmas and proofs are entirely standard. The restriction to value-form arguments for $\beta$-reduction does not pose any complications.

  \begin{lemma}
    Single step reduction implies parallel step reduction. If $m \step m'$, then $m \pstep m'$.
  \end{lemma}

  \begin{lemma}
    Parallel reduction satisfies the diamond property. If $m \pstep m_1$ and $m \pstep m_2$ then there exists $m'$ such that $m_1 \pstep m'$ and $m_2 \pstep m'$.
  \end{lemma}

  \begin{corollary}
    The transitive reflexive closure of parallel reduction is confluent. If $m \pstep^* m_1$ and $m \pstep^* m_2$ then there exists $m'$ such that $m_1 \pstep^* m'$ and $m_2 \pstep^* m'$.
  \end{corollary}

  \begin{corollary}
    The definitional equality relation is an equivalence relation.
  \end{corollary}

  \subsubsection{Substitution} \label{subst}
  Though the \textit{substitution} lemma is widely considered a boring and bureaucratic theorem, it is surprisingly hard to design linear typed languages where the \textit{substitution} lemma is admissible. Much of this difficulty arise during the substitution of non-linear expressions. Perhaps the most famous work detailing the issues of substitution is due to Wadler\cite{substitute}. Since computation arise as a consequence of substitution, it is imperative to get it right.

  Generally, the application rule looks similar to the following for languages with linear types.
  \begin{mathpar}
    \inferrule
    { \Gamma \vdash m : A \multimap B \\ 
      \Delta \vdash n : A }
    { \Gamma, \Delta \vdash m\ n : B } 
  \end{mathpar}

  If type $A$ is a non-linear type, then $n$ ought to be used freely. However, it is possible for $n$ to contain linear variables present in $\Delta$. If $m$ is a lambda abstraction $\lambda x.m'$ where $x$ occurs multiple times within $m'$, substitution of $n$ for $x$ will cause duplication of linear variables. One approach for solving this issue is to wrap non-linear values in an explicit modality, unpacking the internal value only when needed\cite{substitute,neel15}. Another is to ban non-linear expressions from containing linear variables\cite{llf,luo,qtt}.

  Our call-by-value semantics resolves the substitution problem without imposing any of the modifications mentioned previously to the typing of application. The crucial realization is the following \textit{value soundness} lemma: non-linear \textit{values} contain no linear variables.

  \begin{lemma}
    Value soundness. If $\Gamma \vdash v \utype A$ then $\Gamma\ \pure$.
  \end{lemma}

  From the \textit{value soundness} lemma, the standard rule for application is admissible. Intuitively, a single copy of each linear resource is used to create a single non-linear value. This value can then be freely duplicated without needing the original resources to generate fresh copies. This is consistent with the common practice of retrieving non-linear values from linear references.

  \begin{lemma}
    Substitution. For $\Gamma_1, x \stype{s} A \vdash m \stype{t} B$ and $\Gamma_2 \vdash v \stype{s} A$, if there exists $\Gamma$ such that $merge\ \Gamma_1\ \Gamma_2\ \Gamma$ is defined, then $\Gamma \vdash [v/x]m \stype{t} [v/x]B$.
  \end{lemma}

  \subsubsection{Type Soundness}
  The following theorems are a direct result of the \textit{substitution} lemma and various canonical-form lemmas. 

  \begin{theorem}
    Progress. For $\epsilon \vdash m \stype{s} A$, either $m$ is a value or there exists $n$ such that $m \step n$.
  \end{theorem}

  \begin{theorem}
    Preservation. For $\Gamma \vdash m \stype{s} A$, if $m \pstep n$ then $\Gamma \vdash n \stype{s} A$.
  \end{theorem}

  \section{Extensions}

  \subsection{Data Types}
  Though it is possible to encode data and propositions directly in the core language using Church-encodings, it is incredibly inconvenient. To address this, our implementation allows users to define inductive data types\cite{inductive} similar to Coq or Agda\cite{agda}. A pattern matching construct\cite{pattern} is defined on inductive types for data elimination. Checking is performed to ensure that non-linear inductive types do not carry linear terms and linear types cannot be used as type indices or parameters.

  Commonly used inductive types are formalized in Figures \ref{dataform} with their Introduction and Elimination rules formalized in Figure \ref{dataintro} and Figure \ref{dataelim} respectively. Reduction for data types obey the same call-by-value semantics as outlined in \ref{reduction}. Other standard data types that are not covered here are $\mathbb{N}$ for natural numbers and $\top$ for the unit type.

  \begin{figure}[H]
    \caption{Data Formation}
    \begin{mathpar}
      \inferrule
      { \Gamma\ \pure \\ 
        \Gamma \vdash A \utype U \\ 
        \Gamma, x \utype A \vdash B \utype U }
      { \Gamma \vdash \Sigma x:A.B \utype U }
      \rname{$\Sigma$}

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash A \utype U \\ 
        \Gamma, x \utype A \vdash B \utype L }
      { \Gamma \vdash F x:A.B \utype L }
      \rname{$F$}

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash A \utype L \\ 
        \Gamma \vdash B \utype L }
      { \Gamma \vdash A \otimes B \utype L }
      \rname{$\otimes$}

      \inferrule
      { \Gamma\ \pure \\ 
        \Gamma \vdash m \utype A \\
        \Gamma \vdash n \utype A }
      { \Gamma \vdash m =_A n \utype U }
    \end{mathpar}
    \label{dataform}
  \end{figure}

  \begin{figure}[H]
    \caption{Data Introduction}
    \begin{mathpar}
      \inferrule
      { \Gamma_1 \vdash m \utype A \\
        \Gamma_2 \vdash n \utype (\lambda x.B)\ m \\
        merge\ \Gamma_1 \Gamma_2 \Gamma }
      { \Gamma \vdash (m, n) \utype \Sigma x:A.B }
      \rname{$\Sigma$-intro}

      \inferrule
      { \Gamma_1 \vdash m \utype A \\
        \Gamma_2 \vdash n \ltype (\lambda x.B)\ m \\
        merge\ \Gamma_1 \Gamma_2 \Gamma }
      { \Gamma \vdash [m, n] \ltype F x:A.B }
      \rname{$F$-intro}

      \inferrule
      { \Gamma_1 \vdash m \ltype A \\
        \Gamma_2 \vdash n \ltype B \\
        merge\ \Gamma_1 \Gamma_2 \Gamma }
      { \Gamma \vdash \langle m, n \rangle \ltype A \otimes B }
      \rname{$\otimes$-intro}

      \inferrule
      { \Gamma\ \pure \\ \Gamma \vdash n \utype A }
      { \Gamma \vdash \refl\ n \utype n =_A n }
    \end{mathpar}
    \label{dataintro}
  \end{figure}

  \begin{figure}[H]
    \caption{Data Elimination}
    \begin{mathpar}
      \inferrule
      { \Gamma_1 \vdash m \utype \Sigma x.A.B \\
        \Gamma_2, x \utype A, y \utype B \vdash n \stype{s} C \\
        merge\ \Gamma_1 \Gamma_2 \Gamma }
      { \Gamma \vdash \letin{(x , y)}{m}{n} \stype{s} C }
      \rname{$\Sigma$-elim}

      \inferrule
      { \Gamma_1 \vdash m \ltype F x.A.B \\
        \Gamma_2, x \utype A, y \ltype B \vdash n \stype{s} C \\
        merge\ \Gamma_1 \Gamma_2 \Gamma }
      { \Gamma \vdash \letin{[x , y]}{m}{n} \stype{s} C }
      \rname{$F$-elim}

      \inferrule
      { \Gamma_1 \vdash m \ltype A \otimes B \\
        \Gamma_2, x \ltype A, y \ltype B \vdash n \stype{s} C \\
        merge\ \Gamma_1 \Gamma_2 \Gamma }
      { \Gamma \vdash \letin{$\langle x , y\rangle$}{m}{n} \stype{s} C }
      \rname{$\otimes$-elim}

      \inferrule
      { \Gamma_1 \vdash p \utype m =_A n \\
        \Gamma_2 \vdash q \stype{s} B[m] \\
        \overline{\Gamma}, x \utype A \vdash B[x] \utype s \\
        merge\ \Gamma_1 \Gamma_2 \Gamma }
      { \Gamma \vdash \subst(\lambda x.B, p, q) \stype{s} B[n] }
    \end{mathpar}
    \label{dataelim}
  \end{figure}

  \subsection{Imperative Programming}
  With the addition of data types, axioms for imperative programming can be added to the language. In Figure \ref{state} we give a set of axioms for stateful programs. Note that these axioms are not associated with any reductions. This unsurprisingly breaks the \textit{Progress Theorem} as axioms are neither values nor can they reduce. Despite this obvious shortcoming, the axiomatic treatment of state provides an interface for extraction of imperative code with safe manual memory management.

  \subsubsection{State Programs}
  \begin{figure}[H]
    \caption{State Programs}
    \begin{mathpar}
      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash A \utype U \\ 
        \Gamma \vdash m \utype A \\
        \Gamma \vdash l \utype \mathbb{N} }
      { \Gamma \vdash l \mapsto_A m \utype L }
      \rname{Capability}

      \inferrule
      { \Gamma\ \pure \\ 
        \Gamma \vdash A \utype U \\ 
        \Gamma \vdash m \utype A }
      { \Gamma \vdash \new(m) \ltype F l:\mathbb{N}. l \mapsto_A m }
      \rname{New}

      \inferrule
      { \Gamma\ \pure \\
        \Gamma \vdash c \ltype l \mapsto_A m }
      { \Gamma \vdash \free(c) \utype \top }
      \rname{Free}

      \inferrule
      { \Gamma \vdash c \ltype l \mapsto_A m }
      { \Gamma \vdash \get(l, c) \ltype
        F x:A. F e: (x =_A m). l \mapsto_A m }
      \rname{Get}

      \inferrule
      { \Gamma \vdash c \ltype l \mapsto_A m \\
        \overline{\Gamma} \vdash n \utype A }
      { \Gamma \vdash \set(l, c, n) \ltype l \mapsto_A n }
      \rname{Set}
    \end{mathpar}
    \label{state}
  \end{figure}

  \subsubsection{Laws for Free}

  \section{Future Work}
  All of the theorems we have developed so far are purely syntactic in nature. But the natural correspondence between linear types and call-by-value semantics seem to hint at a deeper connection. Indeed, past works\cite{cbvsemantics} have provided insight for the simply typed case. We would like to extend these results our dependent linear type theory.

  \section{Conclusion}

\bibliographystyle{acm}
\bibliography{ref}

\end{document}